{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to neurons\n",
    "\n",
    "At this point we know how to build models and have a computer automatically learn how to match the model to data. That is the core of how any machine learning method works. \n",
    "\n",
    "Now let's narrow our focus and look at **neural nets**. Neural nets are a specific choice of a **model**. It's a network of **neurons** which begs the question, what is a neuron?\n",
    "\n",
    "### Models with multiple inputs\n",
    "\n",
    "So far, we have been using the sigmoid function as our model. One of the forms of the sigmoid function we've used is\n",
    "\n",
    "$$\\sigma_{w, b}(x) = \\frac{1}{1 + \\exp(-wx + b)}$$\n",
    "\n",
    "where `x` and `w` are both single numbers. We have been using this function to model how the amount of the color green in an image (`x`) can be used to determine if an image shows an apple or a banana. \n",
    "\n",
    "But what if we had multiple data features we wanted to fit? \n",
    "\n",
    "We could then extend our model to include multiple features like\n",
    "\n",
    "$$\\sigma_{\\mathbf{w},b}(\\mathbf{x}) = \\frac{1}{1 + \\exp(-w_1 x_1 - w_2 x_2 - \\ldots - w_n x_n + b)}$$\n",
    "\n",
    "Note that now $\\mathbf{x}$ and $\\mathbf{w}$ are vectors with many components rather than a number. \n",
    "\n",
    "For example, $x_1$ might be the amount of the color green in an image, $x_2$ could be the height of the object in the picture, $x_3$ could be the width, and so forth. We can add information for as many features as we have! Our model now has more parameters, but the same idea of gradient descent (\"rolling the ball downhill\") will still work to train our model.\n",
    "\n",
    "This version of the sigmoid model that takes multiple inputs is an example of a **neuron**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the video, we see that one huge class of learning techniques is based around neurons, that is, *artificial neurons*. These are caricatures of real, biological neurons. Both *artificial* and *biological* neurons have several inputs $x_1, \\ldots, x_n$, and a single output, $y$. Schematically they look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "include(\"draw_neural_net.jl\")\n",
    "\n",
    "number_inputs, number_neurons = 4, 1\n",
    "\n",
    "draw_network([number_inputs, number_neurons])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should read this as showing how information flows from left to right: \n",
    "- 4 pieces of input information arrive (shown in green on the left)\n",
    "\n",
    "- the neuron (shown in red on the right) receives all the inputs, processes them, and outputs a single number towards the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, a neuron is just a type of function that takes multiple inputs and returns a single output.\n",
    "\n",
    "The simplest interesting case that we will look at in this notebook is when there are just two pieces of input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "draw_network([2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each link between circles above represents a **weight** $w$ that can be modified to allow the neuron to learn, so in this case the neuron has two weights, $w_1$ and $w_2$. \n",
    "\n",
    "The neuron also has a single bias $b$, and an **activation function**, which we will take to be the $\\sigma$ sigmoidal function that we have been using. (Note that other activation functions can be used!)\n",
    "\n",
    "Let's call our neuron $f_{w_1,w_2, b}(x_1, x_2)$ where\n",
    "\n",
    "$$f_{w_1,w_2, b}(x_1, x_2) = \\sigma(w_1 x_1 + w_2 x_2 + b).$$\n",
    "\n",
    "Note that $f_{w_1,w_2, b}(x_1, x_2)$ has 3 parameters (two weights and a bias).\n",
    "\n",
    "To simplify the notation, and to prepare for more complicated scenarios later, we put the two weights into a vector, and the two data values into another vector:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix};\n",
    "\\qquad\n",
    "\\mathbf{w} = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "We thus have\n",
    "\n",
    "$$f_{\\mathbf{w}, b}(\\mathbf{x}) = \\sigma(\\mathbf{w} \\cdot \\mathbf{x} + b),$$\n",
    "\n",
    "where the dot product $\\mathbf{w} \\cdot \\mathbf{x}$ is an abbreviated syntax for $w_1 x_1 + w_2 x_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "\n",
    "Declare the function `f(x, w, b)` in Julia. `f` should take vectors `x` and `w` as vectors and `b` as a scalar. Furthermore `f` should call\n",
    "\n",
    "```julia\n",
    "Ïƒ(x) = 1 / (1 + exp(-x))\n",
    "```\n",
    "\n",
    "What output do you get for\n",
    "\n",
    "```julia\n",
    "f(3, 4, 5)\n",
    "```\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
