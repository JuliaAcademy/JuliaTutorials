{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning with a single neuron\n",
    "\n",
    "In this notebook, we'll build a neuron that classifies an image as an apple or as a banana using multiple features from the image. We'll **train** our neuron using data from many images; our neuron will thereby **learn** what parameters to use by minimizing a cost function via an algorithm that uses gradient descent.\n",
    "\n",
    "We'll do this with a neuron that takes two inputs, which we can visualize via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "include(\"draw_neural_net.jl\")\n",
    "number_inputs, number_neurons = 2, 1\n",
    "draw_network([number_inputs, number_neurons])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, we need to work with and *clean* some real data. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in some real data! We'll use data that we have prepared from photos of apples and bananas, and it turns out to be stored on disk in data files as \"tab-separated values\". We can read this data in with the `CSV.jl` package, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using CSV\n",
    "using TextParse\n",
    "\n",
    "applecols, applecolnames = TextParse.csvread(\"data/Apple_Golden_1.dat\", '\\t')\n",
    "bananacols, bananacolnames = TextParse.csvread(\"data/bananas.dat\", '\\t');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to use `DataFrames` to store the information from our CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way we can create a `DataFrame` is to pass a dictionary that contain arrays as values and descriptive names for those arrays as keys to the `DataFrame` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apples = DataFrame(Dict(strip(name)=>col for (name, col) in zip(applecolnames, applecols)))\n",
    "bananas = DataFrame(Dict(strip(name)=>col for (name, col) in zip(bananacolnames, bananacols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we used a \"dictionary comprehension\" to create each `DataFrame`. Here is some code to create a dictionary, `appledict` via a dictionary comprehension:\n",
    "\n",
    "```julia\n",
    "appledict = Dict(strip(name)=>col for (name, col) in zip(applecolnames, applecols))\n",
    "```\n",
    "For now, don't worry about the exact syntax used here. Just note that the dictionary created associates names with arrays, and that we've used a dictionary like this to create a `DataFrame` with named columns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for now, each of the two data sets is stored in a `DataFrame` (from the `DataFrames.jl` package).\n",
    "\n",
    "### Roadmap\n",
    "\n",
    "To create a neuron with two inputs, we will just use two data points for each image, say columns 3 and 4 (the mean amount of red and the mean amount of green in the image) so that each data point $\\mathbf{x}^{(i)}$ is a 2-dimensional vector, and our data points will fall on a two-dimensional plane.\n",
    "\n",
    "Our neuron will therefore take a point in the two-dimensional plane as an input argument and return a single output that **classifies** it as an apple ($0$) or a banana ($1$). \n",
    "\n",
    "To do so, it must \"**learn**\" the correct values of the parameters $\\mathbf{w}$ and $b$. For this learning to take place, we'll need labels for each point identifying it as an apple (0) or a banana (1). These labels will allow us to create a cost function, which will help our algorithm determine if a given choice of parameters is doing a good or a poor job of classifying our fruit images. \n",
    "\n",
    "**So what do we have left to do?**\n",
    "\n",
    "The above might sound complex, but luckily we can break it down into a series of actionable steps:\n",
    "\n",
    "1. Clean our input data (amounts of red and green) to get it into a useable format\n",
    "1. Create a series of labels that we can use to identify correct and incorrect classifications\n",
    "1. Define a cost function\n",
    "1. Implement an algorithm to pick parameters for our neuron by minimizing the cost function\n",
    "1. Use all of the above to train our neuron how to classify images as apples or bananas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: \n",
    "\n",
    "Note that *in general we cannot expect that a single neuron will be adequate for classification.*. \n",
    "\n",
    "If a single neuron struggles to classify our images, we may need to use a more complicated neural network structure (which corresponds to using a more complicated function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually it will be necessary to \"clean\", the data in some way, i.e. pre-process it before it can be used for whichever task you are interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next *meta*-exercise will be to collect all the data from columns 3 and 4 into a *single* Julia vector `x` (of which each entry is itself a vector), and the labels into a single vector `y`. Let's do this in a series of steps!\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "First, let's practice grabbing a single column of a `DataFrame`. To grab a column, you can index into the `DataFrame` with the name of the column you want, passed as a symbol. In Julia, symbols are names preceded by a `:`. For example, we could grab the \"height\" column from `apples` by indexing into `apples` with the symbol, `:height`:\n",
    "\n",
    "```julia\n",
    "apples[:height]\n",
    "```\n",
    "\n",
    "Index into `apples` to get the \"red\" column. What is the type of the object returned? How many entries does it have?\n",
    "\n",
    "A) Array, 5 <br>\n",
    "B) DataArray, 5 <br>\n",
    "C) Array, 64 <br>\n",
    "D) DataArray, 64 <br>\n",
    "E) Array, 492 <br>\n",
    "F) DataArray, 492"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "We can grab an individual entry of a `DataFrame` by specifying the row index of the entry and the column symbol. For example, to access the height of the 4th image of an apple, we would execute\n",
    "\n",
    "```julia\n",
    "apples[4, :height]\n",
    "```\n",
    "\n",
    "How much red is there in the 63rd image of a banana?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "We want to reorganize data from the 3rd and 4th columns of `apples` and `bananas` to put that data in a single array. Let's start by organizing the data from the 3rd and 4th columns of `apples` into a single array, `x_apples`. Create `x_apples` such that there is a single element in `x_apples` for each image in `apples`. The $i^{th}$ element in `x_apples` should be a `Vector`, i.e. a 1D `Array`, with two elements - the amount of red and the amount of blue in the $i^{th}$ image from `apples`.\n",
    "\n",
    "Similarly create the `Array` `x_bananas` using data from `bananas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Next we want to combine the elements of `x_apples` and `x_bananas` into a single array, `xs`. `xs` should contain, first, all the elements of `x_apple2` and then all the elements of `x_bananas`. Use the `vcat` function to create `xs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "If you've gotten this far, our data is in the format we want for learning. Now we need labels! We want to store a label (either a `0` or a `1` for every apple or banana image in our data set in an array called `ys`. Recall that \"0\" refers to an apple and \"1\" refers to a banana.\n",
    "\n",
    "Create an array `ys` where the $i^{th}$ element of `ys` is a `0` if the $i^{th}$ element of `xs` is an apple and where the $i^{th}$ element of `ys` is a `1` if the $i^{th}$ element of `xs` is a banana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "\n",
    "Add data points for all apple and banana images in our data sets to a plot using `scatter`. Plot data points for apples in one color and use a different color for banana data points.\n",
    "\n",
    "Hint: You may want to use the `first` and `last` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Learning\" by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, we see that it should be \"easy\" to find a function that separates the data into bananas on one side and apples on the other: we just need to draw a straight line that divides the two clouds of data. We can do this \"by hand\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, the neuron will learn a function of the form $\\sigma(\\mathbf{w} \\cdot \\mathbf{x} + b)$. We can think of $\\sigma$ classifying based on whether the value of its output argument is less than `0.5` or greater than `0.5`. Use the interactive visualization to find suitable values of $\\mathbf{w}$ and $b$ such that the hyperplane $\\sigma (w_1 x_1 + w_2 x_2 + b) = 0.5$ divides the data. This is the same as the hyperplane for which $w_1 x_1 + w_2 x_2 + b = 0$!(Note that there are many such values!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can solve for $x_2$ via \n",
    "$$x_2 = -(w_1 x_1 + b) / w_2.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@manipulate for w1 in -2:0.01:3, w2 in -2:0.01:3, b in -2:0.01:3\n",
    "    scatter(first.(x_apples), last.(x_apples), m=:cross, label=\"apples\")\n",
    "    ylims!(0.3, 0.66)\n",
    "    xlims!(0.45, 0.75)\n",
    "    scatter!(first.(x_bananas), last.(x_bananas), label=\"bananas\")\n",
    "    plot!(x -> -(w1*x + b) / w2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can the neuron *learn* to classify the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready for our first experience of **machine learning**: we will let the neuron learn automatically by processing data and tuning model parameters accordingly (the process we call \"learning\")!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For given values of the parameters $w_1$, $w_2$ and $b$, the function $f_{\\mathbf{w}, b}$ maps a vector of length $2$ to a number between $0$ and $1$ (due to the definition of $\\sigma$). Now we want to have a neuron *learn* suitable values of these parameters. \n",
    "\n",
    "We want to discover (learn!) the parameters such that $f$ models the relationship between the data we explored above about the fruit images, and outputs a classification of the fruit: $0$ if it corresponds to an apple, and $1$ if it is a banana.\n",
    "\n",
    "So the neuron's input will be a vector of 2 pieces of information about an image; let's call the data about the $i$th image $\\mathbf{x}^{(i)}$.\n",
    "We also are given the information about which type of fruit it is, written as a $0$ for an apple, and a $1$ for a banana; let's call this *desired* output number $y^{(i)}$. When we feed in the $i$th data, $\\mathbf{x}^{(i)}$, we want the neuron to give an output that is *as close as possible* to the desired output $y^{(i)}$; i.e. it should **minimize** the mean-squared distance\n",
    "\n",
    "$$C_i = [f_{\\mathbf{w}, b}(\\mathbf{x}^{(i)}) - y^{(i)} ]^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, now we see a key difference from what we did previously: the neuron should vary its parameters in such a way that it manages to minimize this distance for *all* of the input data, simultaneously!\n",
    "\n",
    "How can we express this mathematically? We once again define a cost function $C(\\mathbf{w}, b)$, which tells us \"how wrong\" we are when the parameters take on the given values, and then **minimize** this cost function with respect to all of its parameters.\n",
    "\n",
    "The right way to take account of all the data at once is to use the \"mean-squared error\" cost function, which is the mean (squared) over all the differences between the output of the network, $f_{\\mathbf{w}, b}(\\mathbf{x}^{(i)})$ on the $i$th data, and the desired output $y^{(i)}$:\n",
    "\n",
    "$$C_\\mathrm{total}(\\mathbf{w}, b) = \\frac{1}{N} \\sum_i C_i = \\frac{1}{N} \\sum_i [f_{\\mathbf{w}, b}(\\mathbf{x}^{(i)}) - y^{(i)} ]^2,$$\n",
    "\n",
    "where $N$ is the total number of data in the training set.\n",
    "\n",
    "Why do we choose this particular cost function? Because the minimum conceivable value of this cost function is $0$ (since it is a sum of squares), and this is reached only when the neural network perfectly predicts the output. If we can find a way to minimize this cost function, we will get as close as possible to this perfect prediction. (In general, we will not be able to get an exact prediction.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing the cost function: *stochastic* gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know how to minimise cost functions on a computer: we just calculate the gradient, and do gradient descent! But here we hit a problem: the function $C_\\mathrm{total}$ has a *lot* of terms, and so calculating the gradient of that function will be time-consuming.\n",
    "\n",
    "Instead, we will use a variant called *stochastic* gradient descent. Here, the idea is that we will not use the complete cost function; instead, at each step we will choose a random data point, number $i$, and do a step of gradient descent for the partial cost function $C_i$ *of that data point*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "\n",
    "Write functions for the partial cost function `C(w, b, x, y)`.\n",
    "\n",
    "To do this, recall\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix};\n",
    "\\qquad\n",
    "\\mathbf{w} = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}\n",
    "\\qquad\n",
    "f_{\\mathbf{w}, b}(\\mathbf{x}) = \\sigma(\\mathbf{w} \\cdot \\mathbf{x} + b),$$\n",
    "\n",
    "and declare `f(x, w, b)` as in notebook 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Write a function for `C`'s gradient `∇C (w, b, x, y)` with respect to the parameters $(w_1, w_2, b)$ (using finite differences). $∇C$ will be a vector with one component per parameter:\n",
    "\n",
    "$$∇C = [\\frac{\\partial C}{\\partial w_1}, \\frac{\\partial C}{\\partial w_2}, \\frac{\\partial C}{\\partial b}]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: \n",
    "\n",
    "Implement stochastic gradient descent in the function `stochastic_gradient_descent(C, w, b, xs, ys, N = 1000)`. Use this function to minimize the function $C_\\mathrm{total}$.\n",
    "\n",
    "The algorithm: For each of `N` steps, randomly select a vector $x^(i)$ from `xs` storing your image data (and corresponding $y^(i)$ from `ys`). Calculate the gradient of the cost function, $C_i$, for this image and update each of the parameters, $p_j$, of $C_i$ according to\n",
    "\n",
    "$$p_j = p_j - 0.01 * ∇C_j$$\n",
    "\n",
    "(Here $j$ signifies the $j^{th}$ parameter of $C$ and similarly the $j^{th}$ component of $∇C$.)\n",
    "\n",
    "`stochastic_gradient_descent` should return the updated values for vector $\\mathbf{w}$ and scalar $b$.\n",
    "\n",
    "Optional: Keep track of the value of $C_\\mathrm{total}$ over time if you want to visualize the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: \n",
    "\n",
    "Use the values of `w` and `b` from the last exercise to see how `f` is classifying a couple of the images in out dataset.\n",
    "\n",
    "In particular, calculate `f` using the 1st and the 900th image in `xs`. For which image is the output of `f` closer to the value of its label?\n",
    "\n",
    "A) The output of `f` for the 1st image in `xs` is closer to its label\n",
    "B) The output of `f` for the 900th image in `xs` is closer to its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that *with sufficient training*, the single neuron is approximately able to learn the function for most of the data, but not all:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: \n",
    "\n",
    "Use the `maximum` function to determine the maximum squared distance of the prediction from the true value (for each image, this formula is $y_i - f_{w, b}(x_i)$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: \n",
    "\n",
    "Use `w` and `b` from stochastic gradient descent to draw the function that the network has learned, as before, as the hyperplane $w_1 x + w_2 y + b = 0$. Overlay this with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the neuron has two input variables and one output variable, we can also use a \"heatmap\" to visualize the value of $f_{\\mathbf{w}, b}(x_1, x_2)$ as a function of the input data pairs $(x_1, x_2)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heatmap(0:0.01:1, 0:0.01:1, (x,y)->f([x,y], w, b))\n",
    "\n",
    "scatter!(first.(x_apples), last.(x_apples), m=:cross)\n",
    "scatter!(first.(x_bananas), last.(x_bananas))\n",
    "xlims!(0.45, 0.75)\n",
    "ylims!(0.35, 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the single neuron has indeed *learned* to separate the data using a hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we have looked only at neurons with 2 inputs. In general, a neuron can have any number $n$ of inputs, each with its own weight, and the action of the neuron is described by the function\n",
    "\n",
    "$$f_{w_1,w_2, \\ldots, w_n,b}(x_1, \\ldots, x_n) = \\sigma(w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n + b),$$\n",
    "\n",
    "where the weights are labelled $w_1, \\ldots, w_n$, and the bias is $b$.\n",
    "\n",
    "As we've seen before, we can use vector notation to generalize what we did for $n=2$:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{pmatrix};\n",
    "\\qquad\n",
    "\\mathbf{w} = \\begin{pmatrix} w_1 \\\\ \\vdots \\\\ w_n \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where the **dot product** (or scalar product, or inner product) $\\mathbf{w} \\cdot \\mathbf{x}$ is an abbreviated notation for the sum of the products of the weights with the values:\n",
    "\n",
    "$$ \\mathbf{w} \\cdot \\mathbf{x} := \\sum_i w_i x_i. $$\n",
    "\n",
    "Thus the way that we've written `f`,\n",
    "\n",
    "$$f_{\\mathbf{w}, b}(\\mathbf{x}) = \\sigma(\\mathbf{w} \\cdot \\mathbf{x} + b),$$\n",
    "\n",
    "is general and will apply for neurons with $n$ inputs where $n > 2$. (However, visualizing the higher dimensional datasets on which we might train such neurons will not be as straightforward!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "1",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
